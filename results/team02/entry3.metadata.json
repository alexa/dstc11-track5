{
  "subtask1": {
    "ensemble": "No, only a single model was used for the turn detection",
    "pretrained": "microsoft/deberta-v3-base",
    "external_api": "NIL",
    "desc": "We used the provided baseline"
  },
  "subtask2": {
    "ensemble": "No, only a single model was used for the knowledge selection",
    "pretrained": "microsoft/deberta-v3-base",
    "external_api": "NIL",
    "desc": "We used the provided baseline"
  },
  "subtask3": {
    "ensemble": "Yes, multiple model outputs were combined for the response generation",
    "pretrained": "facebook/bart-base; google/long-t5-tglobal-base; LLaMA 7B LoRA",
    "external_api": "Open AI GPT-4",
    "desc": "[Highest priority for human eval] BART and Long-T5 were trained using the baseline method, LLaMA method described in entry #2. Dialogue history, knowledge snippets, and 3 outputs were rated by GPT-4 on a 1-5 scale. We used the LLaMA output if it scored > 3, otherwise the highest scoring output was selected."
  }
}